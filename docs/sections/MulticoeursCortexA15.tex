\section{Architecture multicœurs avec des processeurs superscalaires in-order (Cortex A15)}
\subsection{Chemin critique, synchronisation et cycles dominants en exécution OpenMP}
De la même manière que dans le cas du Cortex-A7, le goulot d’étranglement se situe dans l’exécution des sections non parallélisables du code ; par conséquent, c’est le \emph{master} qui détermine le maximum de cycles pour chaque exécution, selon les configurations considérées. Cela reste vrai même si ce cœur n’est pas nécessairement celui qui passe le plus de temps dans l’exécution des sections strictement parallèles. On retrouve ainsi la même conclusion que pour l’autre microprocesseur et l’ensemble de ses configurations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../CortexA15/cycles_execution_m16_cortexA15_widths.png}
    \caption{Cortex-A15 (\(m=16\)) : nombre de cycles d’exécution selon le nombre de threads et la largeur du pipeline.}
    \label{fig:cycles_execution_m16_cortex_a15_widths}
\end{figure}

\subsection{Speed-Up per configuration}

Dans le but de disposer d’une perspective de comparaison valide, il est proposé d’analyser le rendement du système en parallélisation dans des conditions comparables à celles du Cortex-A7. Ainsi, on vérifie le comportement pour $1$ à $16$ threads avec des matrices de taille $m=16$, en considérant ici l’effet du \emph{largeur de voie} (\emph{issue width}). Il apparaît que, comme dans le cas du A7, pour ces configurations la fraction de calcul parallélisable n’est pas suffisamment représentative au regard des pénalités mémoire (défauts et latences), des surcoûts liés à l’augmentation du nombre de threads (overhead OpenMP) et du poids des sections linéaires non parallélisables. Par conséquent, les \emph{speedups} ne dépassent pas $1.3$ et, de manière attendue, on observe des \emph{speedups} légèrement plus élevés pour les configurations à largeur de voie plus importante, celles-ci permettant de traiter davantage d’instructions par cycle et donc d’améliorer le débit d’exécution lorsque l’application n’est pas strictement bornée par la mémoire.

Afin de comparer ces résultats à un cas où la complexité de calcul augmente, il a également été proposé, comme pour l’étude du Cortex-A15, d’évaluer la parallélisation de $1$ à $16$ threads pour des largeurs de voie de $2$, $4$ et $8$ sur le produit matriciel avec $m=128$. Les résultats mettent en évidence une dynamique comparable à celle observée pour $m=128$ sur le A7 : un comportement initial relativement proche de l’idéal lorsque l’on augmente le nombre de threads, puis une saturation à partir de $8$ threads, avec un écart croissant à la courbe idéale. Cette saturation s’explique par l’augmentation des opérations mémoire et des défauts de cache, par l’overhead associé à un plus grand nombre de threads, ainsi que par le coût relatif des sections linéaires non parallélisables qui devient plus visible lorsque la partie parallèle se réduit. Enfin, on peut conclure que la configuration à largeur de voie $2$ présente un nombre de cycles supérieur aux autres pour l’ensemble des nombres de threads, et qu’on observe globalement une relation inverse entre le nombre de threads et le nombre de cycles requis par l’application. Néanmoins, cette configuration (voie $2$), étant la moins performante en absolu, se révèle aussi plus sensible aux gains de la parallélisation, ce qui se traduit par des \emph{speedups} plus marqués lorsqu’on augmente le nombre de threads, par rapport aux configurations à $4$ et $8$ voies qui, elles, présentent des dynamiques plus proches l’une de l’autre.


Les courbes de \emph{speedup} (référence à \(1\) thread) pour Cortex-A15 sont présentées pour \(m=16\) et \(m=128\) dans la Figure~\ref{fig:speedup_cortex_a15_m16_m128}.

\begin{figure}[H]
    \centering
    \subfloat[\(m=16\) : \emph{Speedup}]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/speedup_m16_vs_1thread_cortexA15_widths.png}
        \label{fig:speedup_cortex_a15_m16}
    }
    \hfill
    \subfloat[\(m=128\) (jusqu’à 16 threads) : \emph{Speedup}]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/speedup_m128_vs_1thread_cortexA15_widths_t16max.png}
        \label{fig:speedup_cortex_a15_m128_t16}
    }
    \caption{Comparaison du \emph{speedup} pour Cortex-A15 selon la largeur du pipeline (\(m=16\) et \(m=128\)).}
    \label{fig:speedup_cortex_a15_m16_m128}
\end{figure}

Les courbes d’efficacité globale associées aux mêmes configurations sont présentées dans la Figure~\ref{fig:efficacite_cortex_a15_m16_m128}.

\begin{figure}[H]
    \centering
    \subfloat[\(m=16\) : Efficacité globale]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/efficacite_m16_cortexA15_widths.png}
        \label{fig:efficacite_cortex_a15_m16}
    }
    \hfill
    \subfloat[\(m=128\) (jusqu’à 16 threads) : Efficacité globale]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/efficacite_m128_cortexA15_widths_t16max.png}
        \label{fig:efficacite_cortex_a15_m128_t16}
    }
    \caption{Comparaison de l’efficacité globale pour Cortex-A15 selon la largeur du pipeline (\(m=16\) et \(m=128\)).}
    \label{fig:efficacite_cortex_a15_m16_m128}
\end{figure}

\subsection{valeur maximale de l’IPC pour chaque configuration}

Pour Cortex-A15, l’IPC peut être analysé selon deux indicateurs complémentaires : l’IPC maximal observé sur un cœur (\emph{max system.cpu*.ipc}) et l’IPC global de la configuration (\(\mathrm{sim\_insts}/\max(\mathrm{numCycles})\)). Les résultats pour \(m=16\) et \(m=128\), en distinguant les largeurs de voie, sont présentés dans la Figure~\ref{fig:ipc_cortex_a15_m16_m128}.

\begin{figure}[H]
    \centering
    \subfloat[IPC maximal par configuration]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/ipc_max_m16_m128_cortexA15_widths.png}
        \label{fig:ipc_max_cortex_a15_m16_m128}
    }
    \hfill
    \subfloat[IPC global par configuration]{
        \includegraphics[width=0.48\textwidth]{../CortexA15/ipc_global_m16_m128_cortexA15_widths.png}
        \label{fig:ipc_global_cortex_a15_m16_m128}
    }
    \caption{Comparaison des métriques IPC pour Cortex-A15 (\(m=16\) et \(m=128\)).}
    \label{fig:ipc_cortex_a15_m16_m128}
\end{figure}

En termes d’IPC maximal par thread dans la simulation, on observe pour $m=16$ une dynamique similaire à celle discutée précédemment : l’IPC maximal atteint par chacun des threads ne dépend pas directement du nombre de threads utilisés par l’application. En revanche, pour une dimension de matrice $m=128$, plusieurs effets peuvent conduire à une diminution de l’IPC maximal des threads. D’une part, la contention en L2 et au niveau de la mémoire augmente lorsque davantage de cœurs demandent simultanément des données, ce qui se traduit par plus de \emph{misses}, davantage de files d’attente et une latence effective plus élevée ; ainsi, malgré la présence de plus de cœurs, chacun passe proportionnellement plus de temps à attendre les données. D’autre part, la cohérence de cache et le trafic associé (et, dans certains cas, le \emph{false sharing}) peuvent dégrader l’IPC lorsque plusieurs threads accèdent à des données proches partageant les mêmes lignes de cache, provoquant des invalidations et un effet de \emph{ping-pong} coûteux. Enfin, l’overhead d’OpenMP augmente avec le nombre de threads : barrières, répartition du travail et synchronisations représentent un coût relatif plus important, en particulier lorsque la charge utile par thread diminue ; avec $16$ threads et $m=128$, la quantité de travail assignée à chaque thread peut devenir suffisamment faible pour que cet overhead pèse fortement sur l’exécution. Néanmoins, du point de vue de l’IPC global, la dynamique reste comparable à celle observée sur le A7, avec des augmentations quasi linéaires de l’IPC lorsque le nombre de threads croît. De plus, lorsque la dimension de la matrice atteint $m=128$, on obtient des IPC globaux sensiblement plus élevés, ce qui traduit une meilleure exploitation du parallélisme et une contribution plus marquée de l’optimisation des opérations de calcul lorsque le nombre de threads employés augmente.

\subsection{Discussion et interprétation}

Dans un scénario où la charge de calcul est faible, l’intégration de multiples threads produit bien un \emph{speedup}, mais la dynamique de réponse du système devient rapidement dominée par les accès mémoire et par la synchronisation. Cela implique que l’effet d’agrégation de threads est moins significatif en termes de nombre de cycles nécessaires pour réaliser la multiplication matricielle. C’est pourquoi, lorsque la taille de la matrice augmente et que le problème devient plus \emph{compute bound}, on commence à obtenir de meilleurs \emph{speedups}, comme on l’observe également avec le microprocesseur Cortex-A15.

Par ailleurs, l’augmentation de la largeur des voies (par exemple de $2$ à $4$ puis $8$) peut être reliée à une diminution du nombre de cycles requis pour exécuter le produit matriciel, puisque davantage d’instructions peuvent être traitées par cycle. Toutefois, précisément parce que cette optimisation réduit le temps d’exécution de base, les gains relatifs associés à la parallélisation ont tendance à diminuer pour les configurations à $4$ ou $8$ voies : une fraction plus importante du temps total est alors expliquée par des coûts non idéaux (attentes mémoire, cohérence et synchronisation) plutôt que par le calcul utile.

Enfin, on constate que, pour $8$ threads et une matrice de taille $M=128$, les IPC globaux obtenus ne sont que légèrement supérieurs à ceux mesurés sur les configurations de type A7. Cela s’explique par le fait que, compte tenu de la nature de l’opération (produit matriciel) et de la taille considérée, l’exécution devient rapidement \emph{memory bound} et dépend fortement des surcoûts liés à la synchronisation et à l’exécution OpenMP. En conséquence, les avantages micro-architecturaux du A15, mis en évidence lors du premier TP, se trouvent nettement atténués dans ce contexte, ce qui rend la différence entre l’utilisation d’un A7 et d’un A15 relativement peu significative pour cette application.

