\section{Architecture multicœurs avec des processeurs superscalaires in-order (Cortex A7)}

\subsection{Chemin critique, synchronisation et cycles dominants en exécution OpenMP}

En analysant l’architecture multicœur proposée et la manière dont l’algorithme est développé afin de garantir la cohérence des caches, on observe que le processus \emph{master} réalise un travail séquentiel indispensable à l’obtention du résultat final, non parallélisable et donc obligatoire. Ce travail apparaît en une étapes principale , l’initialisation de $A$ et $B$ au moyen de boucles complètes, impliquant davantage d’opérations et, par conséquent, un plus grand nombre d’accès mémoire . Ce traitement n’est pas réparti : il repose sur le cœur exécutant le thread principal. Ainsi, même si les autres cœurs exécutent des calculs en parallèle, ce cœur supporte une « file » de travail supplémentaire. Il convient aussi de noter qu’OpenMP, à la fin d’une région parallèle, impose des barrières de synchronisation : lorsqu’un \emph{worker} termine plus tôt, il reste en attente. 

Dans un programme parallèle, le temps total n’est pas « la somme » des temps de tous les threads ; il correspond au temps nécessaire pour que l’application puisse se terminer. Or, l’application ne peut se terminer que lorsque tous les threads ont fini leur travail . On peut formaliser cette idée par l’équation~\eqref{eq:tapp} :
\begin{equation}
T_{\mathrm{app}} = T_{\mathrm{init}}^{(\mathrm{master})} + \max_{i}\!\left(T_{\mathrm{calc}}^{(\mathrm{thread}\ i)}\right).
\label{eq:tapp}
\end{equation}
Le terme déterminant est le maximum : même si $15$ threads terminent rapidement, si un seul thread prend plus de temps, tous les autres restent bloqués à la synchronisation (\emph{barrier}/\emph{join}) en attendant le dernier. . Ainsi, le master accumule souvent davantage de cycles, car il exécute $T_{\mathrm{init}}^{(\mathrm{master})}$  en plus du calcul parallèle, et il peut aussi inclure des temps d’attente dus aux synchronisations (\emph{barrier}/\emph{join}) ainsi qu’aux effets mémoire et de cohérence. Cela s’accorde avec l’analyse du bus et de la cohérence : lorsque le nombre de cœurs augmente, on observe davantage de lectures sur le bus (matrices partagées $A$ et $B$) et davantage d’invalidations lors des écritures sur $C$, ce qui induit des \emph{stalls} (temps d’arrêt en attente de bus ou d’exclusivité de ligne). 

\subsection{Cycles per configuration}

La Figure~\ref{fig:cycles_per_configuration_cortex_a7} présente le nombre de cycles d’exécution en fonction du nombre de processus exécutés pour les configurations Cortex-A7 disponibles.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../CortexA7/cycles_execution_cortexA7.png}
    \caption{Cortex-A7 : cycles d’exécution de l’application selon le nombre de processus.}
    \label{fig:cycles_per_configuration_cortex_a7}
\end{figure}

Il est proposé de poursuivre l’analyse avec l’estimation du nombre minimal de cycles d’exécution de l’application pour une taille de matrice $M=16$, et ce pour chacune des configurations proposées. Les résultats correspondants sont présentés dans la Figure~\ref{fig:cycles_per_configuration_cortex_a7}. On observe que, même si l’exécution devient plus rapide, le gain de performance n’est pas proportionnel au nombre de threads utilisés, principalement à cause de l’impact des accès mémoire et de la présence des étapes du traitement qui ne sont pas parallélisables et dont le coût devient prépondérant lorsque le temps de calcul parallèle diminue. En particulier, les phases d’initialisation, exécutées séquentiellement par le \emph{master}, constituent une fraction croissante du temps total à mesure que l’optimisation accélère la région de calcul en parallèle. De plus, lorsque le nombre de threads augmente, la quantité d’accès concurrents aux caches partagés (L2/L3) et à la RAM croît également, ce qui génère de la contention et un plus grand nombre de défauts de cache, réduisant ainsi le gain marginal apporté par l’ajout de threads. Par ailleurs, l’écriture concurrente dans la matrice $C$ induit un trafic supplémentaire de cohérence de cache (invalidations et mises à jour de lignes), introduisant des surcoûts absents de la version séquentielle. Enfin, il existe un surcoût propre à OpenMP (création et gestion des threads, ainsi que barrières implicites à la fin du \texttt{parallel for}) qui devient relativement plus significatif lorsque la charge de travail par thread n’est pas suffisamment élevée ou lorsque le \emph{blocking} pour le cache n’est pas mis en œuvre de manière efficace. Par conséquent, le \emph{speedup} observé reflète la combinaison de l’amélioration due au parallélisme et des limites pratiques imposées par la hiérarchie mémoire, ainsi que des coûts de coordination.

Il convient de souligner que, dans le cas où $m$ correspond au nombre de threads, on observe une augmentation du nombre de cycles en passant à $16$ threads par rapport à l’exécution avec $8$ threads. Ce comportement peut s’expliquer par le fait qu’avec $16$ threads, la charge de travail utile par thread devient si faible que le surcoût OpenMP domine l’exécution de la phase parallèle. Cette dégradation est notamment amplifiée par l’augmentation du trafic de cohérence (invalidations et transferts de lignes lors des écritures) ainsi que par une pression accrue sur la mémoire partagée, en particulier le cache L2.
