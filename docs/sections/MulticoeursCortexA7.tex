\section{Architecture multicœurs avec des processeurs superscalaires in-order (Cortex A7)}

\subsection{Chemin critique, synchronisation et cycles dominants en exécution OpenMP}

En analysant l’architecture multicœur proposée et la manière dont l’algorithme est développé afin de garantir la cohérence des caches, on observe que le processus \emph{master} réalise un travail séquentiel indispensable à l’obtention du résultat final, non parallélisable et donc obligatoire. Ce travail apparaît en deux étapes principales : (i) l’initialisation de $A$ et $B$ au moyen de boucles complètes, impliquant davantage d’opérations et, par conséquent, un plus grand nombre d’accès mémoire ; (ii) le remplissage de la matrice $C_x$ pour la validation des résultats générés, ce qui correspond également à une opération volumineuse en mémoire. Ce traitement n’est pas réparti : il repose sur le cœur exécutant le thread principal. Ainsi, même si les autres cœurs exécutent des calculs en parallèle, ce cœur supporte une « file » de travail supplémentaire. Il convient aussi de noter qu’OpenMP, à la fin d’une région parallèle, impose des barrières de synchronisation : lorsqu’un \emph{worker} termine plus tôt, il reste en attente. 

Dans un programme parallèle, le temps total n’est pas « la somme » des temps de tous les threads ; il correspond au temps nécessaire pour que l’application puisse se terminer. Or, l’application ne peut se terminer que lorsque tous les threads ont fini leur travail et que le \emph{master} a exécuté la partie séquentielle finale (par exemple la validation et la terminaison). On peut formaliser cette idée par l’équation~\eqref{eq:tapp} :
\begin{equation}
T_{\mathrm{app}} = T_{\mathrm{init}}^{(\mathrm{master})} + \max_{i}\!\left(T_{\mathrm{calc}}^{(\mathrm{thread}\ i)}\right) + T_{\mathrm{final}}^{(\mathrm{master})}.
\label{eq:tapp}
\end{equation}
Le terme déterminant est le maximum : même si $15$ threads terminent rapidement, si un seul thread prend plus de temps, tous les autres restent bloqués à la synchronisation (\emph{barrier}/\emph{join}) en attendant le dernier. Après cette synchronisation, seul le master continue (validation, opérations de fermeture, etc.). Ainsi, le master accumule souvent davantage de cycles, car il exécute $T_{\mathrm{init}}^{(\mathrm{master})}$ et $T_{\mathrm{final}}^{(\mathrm{master})}$ en plus du calcul parallèle, et il peut aussi inclure des temps d’attente dus aux synchronisations (\emph{barrier}/\emph{join}) ainsi qu’aux effets mémoire et de cohérence. Cela s’accorde avec l’analyse du bus et de la cohérence : lorsque le nombre de cœurs augmente, on observe davantage de lectures sur le bus (matrices partagées $A$ et $B$) et davantage d’invalidations lors des écritures sur $C$, ce qui induit des \emph{stalls} (temps d’arrêt en attente de bus ou d’exclusivité de ligne). 